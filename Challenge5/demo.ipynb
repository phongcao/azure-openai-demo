{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai langchain google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find the temperature first, then use the calculator to raise it to the .023 power.\n",
      "Action: google_search\n",
      "Action Input: \"high temperature in SF yesterday\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mWeather reports from the last weeks in San Francisco with highs and lows. High: 66.2ºf @1:40 PM Low: 55.04ºf @5:56 AM Approx. Precipitation / Rain Total: in. Time (PDT), Temp. (ºf), Humidity (%), Dew Point ( ... San Francisco Weather Forecasts. ... Almanac for June 19, 2023. Forecast. Average *. Record. Temperature. High. 65 °F. 72 °F. 96 °F. Low. 53 °F. 54 °F. Sep 6, 2022 ... Bay Area heat wave: More cities break all-time high temps Tuesday ... recorded an astounding 116 degrees, which it also recorded yesterday. Sep 6, 2022 ... \"Early morning temperatures in the East Bay and higher terrain are running 5 to 10 degrees higher today than they were yesterday,\" Brooke ... Jun 11, 2019 ... San Francisco International Airport yesterday reached its highest temperature ever recorded in the month of June: 100 degrees. Sep 5, 2022 ... In Livermore, the temperature was recorded at 116 degrees - the highest mark the National Weather Service Bay Area has ever recorded in any ... Sep 6, 2022 ... “After record-shattering heat yesterday, today's temperatures are likely ... “Highs inland will rise above 110 degrees, with any exposure to ... Sep 5, 2022 ... 8. Thursday was a record-breaking day for some parts of the Bay Area, with several daily high temperature records. Here's a look at the forecast ... Sep 5, 2022 ... Two Bay Area cities recorded the highest-ever temperature in the region. What's behind the sizzling temperatures? It's called a heat dome: an ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I have the temperature, now I need to use the calculator to raise it to the .023 power.\n",
      "Action: Calculator\n",
      "Action Input: 66.2^.023\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 1.1012343099196273\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The temperature in SF yesterday raised to the .023 power is 1.1012343099196273.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The temperature in SF yesterday raised to the .023 power is 1.1012343099196273.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Agents - LLMs\n",
    "\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "# The language model we're going to use to control the agent.\n",
    "llm = AzureOpenAI(temperature=0, deployment_name=\"testdavinci003\")\n",
    "\n",
    "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"google-search\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    ")\n",
    "\n",
    "# Let's test it out!\n",
    "agent.run(\n",
    "    \"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?<|im_end|>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to find the high temperature in SF yesterday and then raise it to the power of 0.023.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"google_search\",\n",
      "  \"action_input\": \"SF yesterday high temperature Fahrenheit\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mWeather reports from the last weeks in San Francisco with highs and lows. ... Maximum temperature yesterday: 68 °F (at 11:56 am) San Francisco, San Francisco International Airport ... Weather, Sky Cond. Temperature (ºF), Relative Humidity, Wind Chill (°F), Heat San Francisco Weather Forecasts. ... access_time 8:05 PM PDT on June 19, 2023 (GMT -7) | Updated just now. --° | 51°. 58 °F ... TomorrowTue 06/20 High 67 °F. Be prepared with the most accurate 10-day forecast for San francisco, CA with highs, lows, chance of precipitation from The Weather Channel and Weather.com. Temperature (°F), Actual, Historic Avg. Record. High Temp, 57, 71.9, 91. Low Temp, 55, 53.8, 45. Day Average Temp, 56, 62.9, - ... Sep 5, 2022 ... Even cool coastal San Francisco has not been spared the brutal heat, with the National Weather Service issuing a heat advisory for downtown from ... San Francisco, CA Weather Forecast, with current conditions, wind, air quality, and what to ... Current Weather. 10:15 PM. 56°F. RealFeel® 55°. Mostly clear ... See the latest San Francisco weather forecast, current conditions, and live radar. Keep up to date on all San Francisco weather news with KRON4. Hazardous Weather Conditions ... Low: 70 °F. Tuesday. Tuesday: Sunny, with a high near 93. ... Tuesday Night: Partly cloudy, with a low around 67. When a source of heat is available, hold hot foods at 135 degrees Fahrenheit or higher and check the temperature every four hours. If the temperature of the ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe high temperature in SF yesterday was 68°F. Now I need to raise that to the power of 0.023.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"68^0.023\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 1.101914010425215\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe number I raised to the power of 0.023 is 1.101914010425215.\n",
      "\n",
      "Final Answer: 1.101914010425215\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.101914010425215'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Agents - Chat models\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "# First, let's load the language model we're going to use to control the agent.\n",
    "chat = AzureChatOpenAI(temperature=1, deployment_name=\"gpt-35-turbo\")\n",
    "\n",
    "# Note: llm-math might randomly fail when using gpt model due to this known issue\n",
    "# https://github.com/hwchase17/langchain/issues/3071\n",
    "# llm = AzureOpenAI(temperature=0.1, deployment_name=\"gpt35\")\n",
    "\n",
    "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
    "llm = AzureOpenAI(temperature=0, deployment_name=\"testdavinci003\")\n",
    "tools = load_tools([\"google-search\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(\n",
    "    tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    ")\n",
    "\n",
    "# Let's test it out!\n",
    "agent.run(\n",
    "    \"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Hi there! I'm Phong\",\n",
      "  \"history\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:ConversationChain > 2:llm:AzureOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n\\nHuman: Hi there! I'm Phong\\nAI:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:ConversationChain > 2:llm:AzureOpenAI] [1.11s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" Greetings! I am an artificial intelligence designed for friendly conversation. It is nice to meet you, Phong. How may I assist you today? \\n\\n\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 31,\n",
      "      \"prompt_tokens\": 64,\n",
      "      \"total_tokens\": 95\n",
      "    },\n",
      "    \"model_name\": \"gpt-35-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationChain] [1.11s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"response\": \" Greetings! I am an artificial intelligence designed for friendly conversation. It is nice to meet you, Phong. How may I assist you today? \\n\\n\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Can you remind me what my name is?\",\n",
      "  \"history\": \"Human: Hi there! I'm Phong\\nAI:  Greetings! I am an artificial intelligence designed for friendly conversation. It is nice to meet you, Phong. How may I assist you today? \\n\\n\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:ConversationChain > 2:llm:AzureOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\nHuman: Hi there! I'm Phong\\nAI:  Greetings! I am an artificial intelligence designed for friendly conversation. It is nice to meet you, Phong. How may I assist you today? \\n\\n\\nHuman: Can you remind me what my name is?\\nAI:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:ConversationChain > 2:llm:AzureOpenAI] [611.859ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" Your name is Phong! Was I correct? \\n\\n\\n\\n\\n\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 11,\n",
      "      \"prompt_tokens\": 109,\n",
      "      \"total_tokens\": 120\n",
      "    },\n",
      "    \"model_name\": \"gpt-35-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationChain] [613.094ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"response\": \" Your name is Phong! Was I correct? \\n\\n\\n\\n\\n\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is the weather in New York now?\",\n",
      "  \"history\": \"Human: Hi there! I'm Phong\\nAI:  Greetings! I am an artificial intelligence designed for friendly conversation. It is nice to meet you, Phong. How may I assist you today? \\n\\n\\nHuman: Can you remind me what my name is?\\nAI:  Your name is Phong! Was I correct? \\n\\n\\n\\n\\n\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:ConversationChain > 2:llm:AzureOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\nHuman: Hi there! I'm Phong\\nAI:  Greetings! I am an artificial intelligence designed for friendly conversation. It is nice to meet you, Phong. How may I assist you today? \\n\\n\\nHuman: Can you remind me what my name is?\\nAI:  Your name is Phong! Was I correct? \\n\\n\\n\\n\\n\\nHuman: What is the weather in New York now?\\nAI:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:ConversationChain > 2:llm:AzureOpenAI] [2.53s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"  I apologize, but I don't have real-time access to weather data. However, I can give you some general weather information. New York weather-wise is on the cooler side due to its temperate climate. Rain and snow are common during the winter season. The average temperature of New York in winter is 1℃ (33.8℉) and in summer is 26℃ (78.8℉). \\n\\n\\n\\n\\n\\n\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 90,\n",
      "      \"prompt_tokens\": 135,\n",
      "      \"total_tokens\": 225\n",
      "    },\n",
      "    \"model_name\": \"gpt-35-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationChain] [2.53s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"response\": \"  I apologize, but I don't have real-time access to weather data. However, I can give you some general weather information. New York weather-wise is on the cooler side due to its temperate climate. Rain and snow are common during the winter season. The average temperature of New York in winter is 1℃ (33.8℉) and in summer is 26℃ (78.8℉). \\n\\n\\n\\n\\n\\n\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"  I apologize, but I don't have real-time access to weather data. However, I can give you some general weather information. New York weather-wise is on the cooler side due to its temperate climate. Rain and snow are common during the winter season. The average temperature of New York in winter is 1℃ (33.8℉) and in summer is 26℃ (78.8℉). \\n\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Memory - LLMs\n",
    "\n",
    "import langchain\n",
    "\n",
    "from langchain import ConversationChain\n",
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "# Enable debug\n",
    "langchain.debug = True\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    temperature=1,\n",
    "    model_name=\"gpt-35-turbo\",\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    model_kwargs={\"stop\": [\"Human:\", \"AI\"]},\n",
    ")\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "conversation.run(\"Hi there! I'm Phong\")\n",
    "conversation.run(\"Can you remind me what my name is?\")\n",
    "conversation.run(\"What is the weather in New York now?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi there! I'm Phong\n",
      "AI:  Greetings! I am an artificial intelligence designed for friendly conversation. It is nice to meet you, Phong. How may I assist you today? \n",
      "\n",
      "\n",
      "Human: Can you remind me what my name is?\n",
      "AI:  Your name is Phong! Was I correct? \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Human: What is the weather in New York now?\n",
      "AI:   I apologize, but I don't have real-time access to weather data. However, I can give you some general weather information. New York weather-wise is on the cooler side due to its temperate climate. Rain and snow are common during the winter season. The average temperature of New York in winter is 1℃ (33.8℉) and in summer is 26℃ (78.8℉). \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Debug buffer\n",
    "\n",
    "print(conversation.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Hi there! I'm Phong\",\n",
      "  \"history\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:ConversationChain > 2:llm:AzureChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nHuman: Hi there! I'm Phong\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:ConversationChain > 2:llm:AzureChatOpenAI] [758.6220000000001ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Hello Phong! It's good to meet you. How can I help you today?\",\n",
      "        \"generation_info\": null,\n",
      "        \"message\": {\n",
      "          \"content\": \"Hello Phong! It's good to meet you. How can I help you today?\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"example\": false\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 18,\n",
      "      \"prompt_tokens\": 69,\n",
      "      \"total_tokens\": 87\n",
      "    },\n",
      "    \"model_name\": \"gpt-35-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationChain] [759.7669999999999ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"response\": \"Hello Phong! It's good to meet you. How can I help you today?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:ConversationChain > 2:llm:AzureChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nHuman: Hi there! I'm Phong\\nAI: Hello Phong! It's good to meet you. How can I help you today?\\nHuman: Can you remind me what my name is?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:ConversationChain > 2:llm:AzureChatOpenAI] [492.916ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Your name is Phong, just as you told me earlier.\",\n",
      "        \"generation_info\": null,\n",
      "        \"message\": {\n",
      "          \"content\": \"Your name is Phong, just as you told me earlier.\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"example\": false\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 13,\n",
      "      \"prompt_tokens\": 106,\n",
      "      \"total_tokens\": 119\n",
      "    },\n",
      "    \"model_name\": \"gpt-35-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationChain] [493.81199999999995ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"response\": \"Your name is Phong, just as you told me earlier.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:ConversationChain > 2:llm:AzureChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nHuman: Hi there! I'm Phong\\nAI: Hello Phong! It's good to meet you. How can I help you today?\\nHuman: Can you remind me what my name is?\\nAI: Your name is Phong, just as you told me earlier.\\nHuman: What is the weather in New York now?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:ConversationChain > 2:llm:AzureChatOpenAI] [1.55s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"I am not able to check the current weather on my own, as I do not have access to the internet. However, if you provide me with a few moments to search for the information, I can tell you what the current weather is like in New York City.\",\n",
      "        \"generation_info\": null,\n",
      "        \"message\": {\n",
      "          \"content\": \"I am not able to check the current weather on my own, as I do not have access to the internet. However, if you provide me with a few moments to search for the information, I can tell you what the current weather is like in New York City.\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"example\": false\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 54,\n",
      "      \"prompt_tokens\": 138,\n",
      "      \"total_tokens\": 192\n",
      "    },\n",
      "    \"model_name\": \"gpt-35-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationChain] [1.55s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"response\": \"I am not able to check the current weather on my own, as I do not have access to the internet. However, if you provide me with a few moments to search for the information, I can tell you what the current weather is like in New York City.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am not able to check the current weather on my own, as I do not have access to the internet. However, if you provide me with a few moments to search for the information, I can tell you what the current weather is like in New York City.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Memory - Chat models\n",
    "\n",
    "import langchain\n",
    "\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Enable debug\n",
    "langchain.debug = True\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"The following is a friendly conversation between a human and an AI. The AI is talkative and \"\n",
    "            \"provides lots of specific details from its context. If the AI does not know the answer to a \"\n",
    "            \"question, it truthfully says it does not know.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat = AzureChatOpenAI(\n",
    "    temperature=1, model_name=\"gpt-35-turbo\", deployment_name=\"gpt-35-turbo\"\n",
    ")\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "conversation = ConversationChain(memory=memory, prompt=prompt, llm=chat)\n",
    "\n",
    "conversation.run(input=\"Hi there! I'm Phong\")\n",
    "conversation.run(input=\"Can you remind me what my name is?\")\n",
    "conversation.run(input=\"What is the weather in New York now?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content=\"Hi there! I'm Phong\", additional_kwargs={}, example=False), AIMessage(content=\"Hello Phong! It's good to meet you. How can I help you today?\", additional_kwargs={}, example=False), HumanMessage(content='Can you remind me what my name is?', additional_kwargs={}, example=False), AIMessage(content='Your name is Phong, just as you told me earlier.', additional_kwargs={}, example=False), HumanMessage(content='What is the weather in New York now?', additional_kwargs={}, example=False), AIMessage(content='I am not able to check the current weather on my own, as I do not have access to the internet. However, if you provide me with a few moments to search for the information, I can tell you what the current weather is like in New York City.', additional_kwargs={}, example=False)]\n"
     ]
    }
   ],
   "source": [
    "# Debug buffer\n",
    "\n",
    "print(conversation.memory.buffer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
