{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai langchain google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find the temperature first, then use the calculator to raise it to the .023 power.\n",
      "Action: google_search\n",
      "Action Input: \"high temperature in SF yesterday\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mWeather reports from the last weeks in San Francisco with highs and lows. High: 69.8ºf @12:00 PM Low: 57.2ºf @4:10 AM Approx. Precipitation / Rain Total: in. Time (PDT), Temp. (ºf), Humidity (%), Dew Point ( ... San Francisco Weather Forecasts. Weather Underground provides local ... Almanac for June 18, 2023. Forecast. Average *. Record. Temperature. High. Sep 6, 2022 ... Bay Area heat wave: More cities break all-time high temps Tuesday ... recorded an astounding 116 degrees, which it also recorded yesterday. Sep 6, 2022 ... \"Early morning temperatures in the East Bay and higher terrain are running 5 to 10 degrees higher today than they were yesterday,\" Brooke ... Jun 11, 2019 ... San Francisco International Airport yesterday reached its highest temperature ever recorded in the month of June: 100 degrees. Sep 5, 2022 ... In Livermore, the temperature was recorded at 116 degrees - the highest mark the National Weather Service Bay Area has ever recorded in any ... Sep 6, 2022 ... “After record-shattering heat yesterday, today's temperatures are likely ... Area will be found in Walnut Creek and Danville, with a high of ... Sep 5, 2022 ... 8. Thursday was a record-breaking day for some parts of the Bay Area, with several daily high temperature records. Here's a look at the forecast ... Sep 5, 2022 ... Two Bay Area cities recorded the highest-ever temperature in the region. What's behind the sizzling temperatures? It's called a heat dome: an ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I have the temperature, now I need to use the calculator to raise it to the .023 power.\n",
      "Action: Calculator\n",
      "Action Input: 69.8^.023\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 1.1025763550530143\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 1.1025763550530143\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.1025763550530143'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Agents - LLMs\n",
    "\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "# The language model we're going to use to control the agent.\n",
    "llm = AzureOpenAI(temperature=0, deployment_name=\"textdavinci003\")\n",
    "\n",
    "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"google-search\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    ")\n",
    "\n",
    "# Let's test it out!\n",
    "agent.run(\n",
    "    \"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?<|im_end|>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: The first part of the question can be answered with a google search. The second part will require a calculator.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"google_search\",\n",
      "  \"action_input\": \"San Francisco yesterday high temperature fahrenheit\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mWeather reports from the last weeks in San Francisco with highs and lows. ... Maximum temperature yesterday: 68 °F (at 11:56 am) San Francisco, San Francisco International Airport ... Weather, Sky Cond. Temperature (ºF), Relative Humidity, Wind Chill (°F), Heat San Francisco Weather Forecasts. ... TodayMon 06/19 High 64 °F ... Almanac for June 18, 2023. Forecast. Average *. Record. Temperature. High. 67 °F. 72 °F. Weather reports from the last weeks in San Francisco Bay Area with highs and lows. ... Maximum temperature yesterday: 68 °F (at 11:56 am) Temperature (°F), Actual, Historic Avg. Record. High Temp, 57, 71.9, 91. Low Temp, 57, 53.8, 45. Day Average Temp, 57, 62.9, - ... Sep 6, 2022 ... Kentfield also hit its highest temperature since 1950 at 107 degrees Fahrenheit. The San Francisco airport, Oakland Museum, Salinas, ... Almanac for June 18, 2023. Forecast. Average *. Record. Temperature. High. 81 °F. 82 °F. 97 °F. Low. 64 °F. 67 °F. 55 °F. Rain/Snow Depth. Precipitation. Sep 5, 2022 ... Even cool coastal San Francisco has not been spared the brutal heat, with the National Weather Service issuing a heat advisory for downtown from ... San Francisco CA. Tonight. Tonight: Mostly clear, with a low around 53. Breezy, with a west. Mostly Clear and Breezy. Low: 53 °F. Juneteenth. San Francisco, CA Weather Forecast, with current conditions, wind, air quality, and what to expect for the next 3 days.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe maximum temperature in San Francisco yesterday was 68 °F. I will now use a calculator to raise that number to the 0.023 power.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"68^0.023\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 1.101914010425215\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "\n",
      "Final Answer: 1.101914010425215\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.101914010425215'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Agents - Chat models\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "# First, let's load the language model we're going to use to control the agent.\n",
    "chat = AzureChatOpenAI(temperature=1, deployment_name=\"gpt35\")\n",
    "\n",
    "# Note: llm-math might randomly fail when using gpt model due to this known issue\n",
    "# https://github.com/hwchase17/langchain/issues/3071\n",
    "# llm = AzureOpenAI(temperature=0.1, deployment_name=\"gpt35\")\n",
    "\n",
    "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
    "llm = AzureOpenAI(temperature=0, deployment_name=\"textdavinci003\")\n",
    "tools = load_tools([\"google-search\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(\n",
    "    tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    ")\n",
    "\n",
    "# Let's test it out!\n",
    "agent.run(\n",
    "    \"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:ConversationChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Hi there! I'm Phong\",\n",
      "  \"history\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:ConversationChain > 2:RunTypeEnum.llm:AzureOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n\\nHuman: Hi there! I'm Phong\\nAI:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:ConversationChain > 2:RunTypeEnum.llm:AzureOpenAI] [478.043ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" Hello Phong! I'm glad to meet you!\\n\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 64,\n",
      "      \"total_tokens\": 75,\n",
      "      \"completion_tokens\": 11\n",
      "    },\n",
      "    \"model_name\": \"gpt-35-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:ConversationChain] [478.834ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"response\": \" Hello Phong! I'm glad to meet you!\\n\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:ConversationChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Can you remind me what my name is?\",\n",
      "  \"history\": \"Human: Hi there! I'm Phong\\nAI:  Hello Phong! I'm glad to meet you!\\n\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:ConversationChain > 2:RunTypeEnum.llm:AzureOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\nHuman: Hi there! I'm Phong\\nAI:  Hello Phong! I'm glad to meet you!\\n\\nHuman: Can you remind me what my name is?\\nAI:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:ConversationChain > 2:RunTypeEnum.llm:AzureOpenAI] [487.60900000000004ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"  Your name is Phong!\\n\\n\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 89,\n",
      "      \"total_tokens\": 96,\n",
      "      \"completion_tokens\": 7\n",
      "    },\n",
      "    \"model_name\": \"gpt-35-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:ConversationChain] [488.354ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"response\": \"  Your name is Phong!\\n\\n\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:ConversationChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is the weather in New York now?\",\n",
      "  \"history\": \"Human: Hi there! I'm Phong\\nAI:  Hello Phong! I'm glad to meet you!\\n\\nHuman: Can you remind me what my name is?\\nAI:   Your name is Phong!\\n\\n\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:ConversationChain > 2:RunTypeEnum.llm:AzureOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\nHuman: Hi there! I'm Phong\\nAI:  Hello Phong! I'm glad to meet you!\\n\\nHuman: Can you remind me what my name is?\\nAI:   Your name is Phong!\\n\\n\\nHuman: What is the weather in New York now?\\nAI:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:ConversationChain > 2:RunTypeEnum.llm:AzureOpenAI] [832.351ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" Sorry, I do not know the weather in New York at this moment. But I can search on the internet for you if you like.\\n\\n\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 109,\n",
      "      \"total_tokens\": 137,\n",
      "      \"completion_tokens\": 28\n",
      "    },\n",
      "    \"model_name\": \"gpt-35-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:ConversationChain] [832.964ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"response\": \" Sorry, I do not know the weather in New York at this moment. But I can search on the internet for you if you like.\\n\\n\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Sorry, I do not know the weather in New York at this moment. But I can search on the internet for you if you like.\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Memory - LLMs\n",
    "\n",
    "import langchain\n",
    "\n",
    "from langchain import ConversationChain\n",
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "# Enable debug\n",
    "langchain.debug = True\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    temperature=1,\n",
    "    model_name=\"gpt-35-turbo\",\n",
    "    deployment_name=\"gpt35\",\n",
    "    model_kwargs={\"stop\": [\"Human:\", \"AI\"]},\n",
    ")\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "conversation.run(\"Hi there! I'm Phong\")\n",
    "conversation.run(\"Can you remind me what my name is?\")\n",
    "conversation.run(\"What is the weather in New York now?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi there! I'm Phong\n",
      "AI:  Hello Phong! I'm glad to meet you!\n",
      "\n",
      "Human: Can you remind me what my name is?\n",
      "AI:   Your name is Phong!\n",
      "\n",
      "\n",
      "Human: What is the weather in New York now?\n",
      "AI:  Sorry, I do not know the weather in New York at this moment. But I can search on the internet for you if you like.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Debug buffer\n",
    "\n",
    "print(conversation.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:ConversationChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Hi there! I'm Phong\",\n",
      "  \"history\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:ConversationChain > 2:RunTypeEnum.llm:AzureChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nHuman: Hi there! I'm Phong\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:ConversationChain > 2:RunTypeEnum.llm:AzureChatOpenAI] [669.1049999999999ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Hello Phong! It's nice to meet you. How can I assist you today?\",\n",
      "        \"generation_info\": null,\n",
      "        \"message\": {\n",
      "          \"content\": \"Hello Phong! It's nice to meet you. How can I assist you today?\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"example\": false\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 18,\n",
      "      \"prompt_tokens\": 69,\n",
      "      \"total_tokens\": 87\n",
      "    },\n",
      "    \"model_name\": \"gpt-35-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:ConversationChain] [670.088ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"response\": \"Hello Phong! It's nice to meet you. How can I assist you today?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:ConversationChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:ConversationChain > 2:RunTypeEnum.llm:AzureChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nHuman: Hi there! I'm Phong\\nAI: Hello Phong! It's nice to meet you. How can I assist you today?\\nHuman: Can you remind me what my name is?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:ConversationChain > 2:RunTypeEnum.llm:AzureChatOpenAI] [551.184ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Certainly! Your name is Phong.\",\n",
      "        \"generation_info\": null,\n",
      "        \"message\": {\n",
      "          \"content\": \"Certainly! Your name is Phong.\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"example\": false\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 8,\n",
      "      \"prompt_tokens\": 106,\n",
      "      \"total_tokens\": 114\n",
      "    },\n",
      "    \"model_name\": \"gpt-35-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:ConversationChain] [552.153ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"response\": \"Certainly! Your name is Phong.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:ConversationChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:ConversationChain > 2:RunTypeEnum.llm:AzureChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nHuman: Hi there! I'm Phong\\nAI: Hello Phong! It's nice to meet you. How can I assist you today?\\nHuman: Can you remind me what my name is?\\nAI: Certainly! Your name is Phong.\\nHuman: What is the weather in New York now?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:ConversationChain > 2:RunTypeEnum.llm:AzureChatOpenAI] [1.48s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"I'm sorry, but I don't have access to real-time weather data. However, according to the latest weather reports, New York is experiencing a mix of sun and clouds with temperatures in the mid-60s Fahrenheit. Would you like me to look up more information about the weather in New York?\",\n",
      "        \"generation_info\": null,\n",
      "        \"message\": {\n",
      "          \"content\": \"I'm sorry, but I don't have access to real-time weather data. However, according to the latest weather reports, New York is experiencing a mix of sun and clouds with temperatures in the mid-60s Fahrenheit. Would you like me to look up more information about the weather in New York?\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"example\": false\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 61,\n",
      "      \"prompt_tokens\": 133,\n",
      "      \"total_tokens\": 194\n",
      "    },\n",
      "    \"model_name\": \"gpt-35-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:ConversationChain] [1.48s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"response\": \"I'm sorry, but I don't have access to real-time weather data. However, according to the latest weather reports, New York is experiencing a mix of sun and clouds with temperatures in the mid-60s Fahrenheit. Would you like me to look up more information about the weather in New York?\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I don't have access to real-time weather data. However, according to the latest weather reports, New York is experiencing a mix of sun and clouds with temperatures in the mid-60s Fahrenheit. Would you like me to look up more information about the weather in New York?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Memory - Chat models\n",
    "\n",
    "import langchain\n",
    "\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Enable debug\n",
    "langchain.debug = True\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"The following is a friendly conversation between a human and an AI. The AI is talkative and \"\n",
    "            \"provides lots of specific details from its context. If the AI does not know the answer to a \"\n",
    "            \"question, it truthfully says it does not know.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat = AzureChatOpenAI(\n",
    "    temperature=1, model_name=\"gpt-35-turbo\", deployment_name=\"gpt35\"\n",
    ")\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "conversation = ConversationChain(memory=memory, prompt=prompt, llm=chat)\n",
    "\n",
    "conversation.run(input=\"Hi there! I'm Phong\")\n",
    "conversation.run(input=\"Can you remind me what my name is?\")\n",
    "conversation.run(input=\"What is the weather in New York now?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content=\"Hi there! I'm Phong\", additional_kwargs={}, example=False), AIMessage(content=\"Hello Phong! It's nice to meet you. How can I assist you today?\", additional_kwargs={}, example=False), HumanMessage(content='Can you remind me what my name is?', additional_kwargs={}, example=False), AIMessage(content='Certainly! Your name is Phong.', additional_kwargs={}, example=False), HumanMessage(content='What is the weather in New York now?', additional_kwargs={}, example=False), AIMessage(content=\"I'm sorry, but I don't have access to real-time weather data. However, according to the latest weather reports, New York is experiencing a mix of sun and clouds with temperatures in the mid-60s Fahrenheit. Would you like me to look up more information about the weather in New York?\", additional_kwargs={}, example=False)]\n"
     ]
    }
   ],
   "source": [
    "# Debug buffer\n",
    "\n",
    "print(conversation.memory.buffer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
